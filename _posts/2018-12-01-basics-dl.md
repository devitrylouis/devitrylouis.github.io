---
title: 'Basics of Deep Learning'
date: 2018-12-01
permalink: /posts/2018/11/basics-d/
tags:
  - Deep Learning
  - Basics
---

# Activation functions

<b>Logistic Sigmoid:</b> $g(z) = \frac{1}{1 + exp(-z)}$
<b>Hyperbolic Tangent Units:</b> $g(z) = tanh(z)$

- Functions used in the first neural networks
- Can saturate easily
- tanh resembles the identity function around 0
- Can still be useful (for robustness to light changes, for example)

Rectified Linear Units (ReLU)

Activation function:
g(a) = max (0, a)

Active units do not saturate.
Gradient is constant, second derivatives (almost) zeros.
(Non-differentiability at 0 is not a problem in practice)


<b>Generalizations of ReLU:</b> perform sometimes better.
- <u>Absolute value rectification:</u> $g(z) = \mid z\mid$
Can be useful for features invariant to polarity change.
- <u>Leaky ReLU:</u> $g(z) = max {0, z} + \alpha min {0, z} $
Fixed, very small $(~0.001) \alpha$
- <u>Parametric ReLU (PReLU):</u> $g(z) = max {0, z} + \alpha min {0, z}$ where $\alpha$ is learned

# Loss Function

## Negative log-likelihood
Maximizing the likelihood / minimizing the negative log- likelihood:

$$
J(\theta) = - \mathbb{E}_{(x, y) \sim p_{data}} log p_{model}(y\mid x)
$$

where:
• $x$: network input
• $y$: $x$'s label / expected value for $x$
• $p_{data}$ is the distribution of $(x, y)$ for a training set.
• $p_{model}(y \mid x):$ how we compute the probability for a value $y$ from the network output for $x$

<i>Note:</i> Negative log-likelihood = cross-entropy between the training data and the model distributions.

## Mean Squared Error

If we choose p_{model}(y\mid x) = \mathcal{N}(y; f(x, \theta), I), the negaive log-likelihood becomes:

$$
J(\theta) = - \mathbb{E}_{(x, y) \sim p_{data}} \mid\mid y-f(x;\theta) \mid\mid^{2}
$$

------
