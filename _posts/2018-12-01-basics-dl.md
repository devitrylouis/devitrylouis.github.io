---
title: 'Introduction to Deep Learning'
date: 2018-12-01
permalink: /posts/2018/11/basics-dl/
tags:
  - Deep Learning
  - Basics
---

# History of Deep Learning

## Perceptron

<b>Key idea: (1958, Frank Rosenblatt)</b>
1. Data are represented as vectors
2. <i>Collect training data:</i> some are positive examples, some are negative examples
3. <i>Training:</i> find $a$ and $$ so that
    * $a > x + b$ is positive for positive samples $x$
    * $a > x + b$ is negative for negative samples $x$
4. <i>Testing:</i> the perceptron can now classify new examples.

<i>Notes:</i>
- This is not always possible to satisfy for all the samples
- Intuitively, it is equivalent to finding a separating hyperplane

<b>Training the perceptron:</b> At the time, ad hoc algorithm:
1. Start from a random initialization
2. For each training sample x:
    * compare the value of $a > x + b$ and its expected sign
    * adapt a and b to get a better value for $a > x + b$

<i>Note:</i> The perceptron is roughly inspired from the neuron:

![Perceptron vs. Neuron](https://appliedgo.net/media/perceptron/neuron.png)

<b>Limitations:</b> 1969, Perceptrons book, Minsky and Papert
- A perceptron can only classify data points that are linearly separable:
- Fail easy case such as the x-or function

<i>Consequence:</i> It is seen by many as a justification to stop research on perceptrons and entails the "AI winter" of the 1970s.



## Multilayer Perceptron/Neural Network



------
