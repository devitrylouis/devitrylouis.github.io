---
title: 'Optimizing Deep Learning'
date: 2018-12-01
permalink: /posts/2018/11/optimize-dl/
tags:
  - Deep Learning
---

First-order methods: gradient descent and variants.

# Local Minima

Convex optimization => The problem can be reduced to finding a local minimum.

When optimizing a convex function, we know that we have reached a good solution if we ﬁnd a critical point of any kind.

Nearly all (Extreme ML) Neural Nets are not convex, but guaranteed to have an extremely large number of local minima.
Not that problematic

- Model identiﬁability = model has a suﬃciently large training set that can rule out all but one setting of the model’s parameters.

- Models with latent variables are often not identiﬁable because we can obtain equivalent models by exchanging latent variables with each other.

If we have m layers with n units each, then there are $n!^{m}$ ways of arranging the hidde n units. This kind of non identiﬁability is known as weight space symmetry.

In addition to <b>weight space symmetry</b>, many kinds of neural networks have additional causes of non identiﬁability. For example, in any rectiﬁed linear or maxout network, we can scale all the incoming weights and biases of a unit by $\alpha$ if we also scale all its outgoing weights by 1/α.

This means that—if the cost function does not include terms such as weight decay that depend directly on the weights rather than the models’ outputs — every local minimum of a rectiﬁed linear or maxout network lies on an (m ×n)-dimensional hyperbola of equivalent local minima. These model identiﬁability issues mean that a neural network cost function can have an extremely large or even uncountably inﬁnite amount of local minima. However, all these local minima arising from nonidentiﬁability are equivalent to each other in cost function value. As a result, these local minima are not a problematic form of non convexity. Local minima can be problematic if they have high cost in comparison to the global minimum. One can construct small neural networks, even without hidden units, that have local minima with higher cost than the global minimum.

Local minima with high costs => Problematic for gradient-based optimization algorithms

Open questions:
- Neural Netw have many local minima of high cost?

- Do optimization algorithms encounter?

Today, experts suspect that,for suﬃciently large neural networks, most local minima have a low cost function value, and that it is not important to ﬁnd a true global minimum rather than to ﬁnd a point in parameter space that has low but not minimal cost

TEST to RULE OUT loacl minima:
- Plotting the norm of the gradient over time.
    - Norm of the gradient does not shrink to insigniﬁcant size => the problem is neither local minima nor any other kind of critical point.
    - In high-dimensional spaces, positively establishing that local minima are the problem can be very diﬃcult. Many structures other than local minima also have small gradients!

# Topology

<b>Saddle Points:</b> In high dimensions, saddle points are more likely than local minima as some eigenvalues are positive, some are negative.

For $\theta^{*}$ to be a local minimum, we need:
- \mid\mid \frac{\partial J}{\partial \theta} (\theta^{*}) \mid\mid = 0
- All eigenvalues of $\frac{\partial^{2}J}{\partial \theta^{2}}(\theta^{*})$ are positive

For random functions in n dimensions, the probability for all the eigenvalues to be all negative is $1 / n$.

# Plateaus, Saddle Points and Other Flat Regions

High-dimensional non convex functions, local minima (and maxima) are in fact rare compared to another kind of point with zero gradient (saddlepoint)

At a saddle point, the Hessian matrix has both positive and negative eigenvalues. Points lying along eigenvectors associated with  positive eigenvalues have greater cost than the saddle point, while points lying along negative eigenvalues have lower value. We can think of a saddle point as being a local minimum along one cross-section of the cost function and a localmaximum along another cross-section. See ﬁgure 4.5 for an illustration. Many classes of random functions exhibit the following behavior: in low-dimensional spaces, local minima are common. In higher-dimensional spaces, local minima are rare, and saddle points are more common.

For a function f:Rn→ R of this type, the expected ratio of the number of saddle points to local minima grows exponentially with n. To understand the intuition behind this behavior, observe that the Hessian matrix at a local minimum has only positive eigenvalues. The Hessian matrix at a saddle point has a mixture of positive and negative eigenvalues. Imagine that the sign of each eigenvalue is generated by ﬂipping a coin.

In a single dimension, it is easy to obtain a local minimum by tossing a coin and getting heads once. In n-dimensional space, it is exponentially unlikely that all n coin tosses will be heads. See Dauphin et al. (2014) for a review of the relevant theoretical work. An amazing property of many random functions is that the eigenvalues of the Hessian become more likely to be positive as we reach regions of lower cost. In our coin tossing analogy, this means we are more likely to have our coin come up heads n times if we are at a critical point with low cost. It also means that local minima are much more likely to have low cost than high cost. Critical points withhigh cost are far more likely to be saddle points. Critical points with extremely high cost are more likely to be local maxima.This happens for many classes of random functions. Does it happen for neural networks? Baldi and Hornik (1989) showed theoretically that shallow autoencoders(feedforward networks trained to copy their input to their output, described inchapter 14) with no nonlinearities have global minima and saddle points but no local minima with higher cost than the global minimum. They observed withoutproof that these results extend to deeper networks without nonlinearities. Theoutput of such networks is a linear function of their input, but they are usefulto study as a model of nonlinear neural networks because their loss function isa nonconvex function of their parameters. Such networks are essentially justmultiple matrices composed together. Saxe et al. (2013) provided exact solutionsto the complete learning dynamics in such networks and showed that learning in these models captures many of the qualitative features observed in the training ofdeep models with nonlinear activation functions. Dauphin et al. (2014) showedexperimentally that real neural networks also have loss functions that contain verymany high-cost saddle points. Choromanska et al. (2014) provided additionaltheoretical arguments, showing that another class of high-dimensional randomfunctions related to neural networks does so as well.What are the implications of the proliferation of saddle points for trainingalgorithms? For ﬁrst-order optimization, algorithms that use only gradient infor-mation, the situation is unclear. The gradient can often become very small near asaddle point. On the other hand, gradient descent empirically seems able to escapesaddle points in many cases. Goodfellow et al. (2015) provided visualizations ofseveral learning trajectories of state-of-the-art neural networks, with an examplegiven in ﬁgure 8.2. These visualizations show a ﬂattening of the cost function neara prominent saddle point, where the weights are all zero, but they also show thegradient descent trajectory rapidly escaping this region. Goodfellow et al. (2015)283

For more informations on the Hessian, go [here](/posts/2018/11/basics-optimization/https://devitrylouis.github.io).

# Gradient descent

## Back propagation

## Stochastic Gradient Descent

## Momentum

## Adagrad

## RMSProp

## Adam

# Practical DL

Ill-conditioning of the Hessian Matrix is a major problem met when optimizing convex functions (believed to be present in neural network training problems)


### Math

Recall from equation 4.9 that a second-order Taylor series expansion of the cost function predicts that a gradient descent step of $−\epsilon g$ will add

$$
\frac{1}{2} \epsilon^{2}g^{T}Hg - \epsilon g^{T}g
$$

to the cost. Ill-conditioning of the gradient becomes a problem when $\frac{1}{2} \epsilon^{2}g^{T}Hg$ exceeds $\epsilon g^{T}g$.

### Implications
- The result is that learning becomes very slow despite the presence of a strong gradient because the learning rate must be shrunk to compensate for even stronger curvature.
- Cause SGD to get “stuck” (even very small steps increase the cost function)

### How to solve it?
- One can monitor the squared gradient norm $g^{T}g$ and the $g^{T}Hg$ term.
    - The gradient norm does not shrink signiﬁcantly throughout learning, but the $g^{T}Hg$ term grows by more than an order of magnitude.

### Note for deep learning

Some of the techniques used to combat it in other contexts are less applicable to neural networks (Newton’s method is good for requires signiﬁcant modiﬁcations before it can be applied to neural networks).



------
