---
title: 'Linear Discriminant Analysis'
date: 2018-10-16
permalink: /posts/2018/10/ml-lda/
tags:
  - Machine Learning
---
LDA is closely related to PCA as it is linear.

- PCA, the transformation is baed on minimizing mean square error between original data vectors and data vectors that can be estimated fro the reduced dimensionality data vectors

- LDA, the transformation is based on maximizing a ratio of “between-class variance” to “within-class variance” with the goal of reducing data variation in the same class and increasing the separation between classes.

Here is the graphic intuition:

![LDA vs PCA](https://sebastianraschka.com/images/blog/2014/linear-discriminant-analysis/lda_1.png)

<b>Goals:</b>
1. Obtain a scalar y by projecting the samples X onto a line:

$$y = \beta^{T} X$$

2. Find $\beta^{*}$ to maximize the ratio of “between-class variance” to “within-class variance”.

## 1. Two classes problem

### 1.1. Head the problem

<b>Mean-class vector:</b> Mean vector of each class with its projection

$$
\begin{align}
u_{k} &= \frac{1}{N_{k}}\sum_{i\in C_{k}} x^{(i)}\\
\hat{u}_{k} &= \beta^{T}u_{k}
\end{align}
$$

<b>Between-class variance:</b> a measure of separation between two classes is to choose the distance between the projected means, which is in y-space

$$
\hat{u}_{2} - \hat{u}_{1} = \beta^{T}(u_{2} - u_{1})
$$

<b>Within-class variance:</b> Variance of the elements of the $k$-th class relative to its mean

$$
\hat{s}_{k}^{2} = \sum_{i \in C_{k}} (y^{(i)}-\hat{u}_{k})^{2}
$$

<b>Objective function:</b> We are looking for a projection where examples from the class are projected very close to each other and at the same time, the projected means are as farther apart as possible. Naturally:
$$
J(\beta) = \frac{(\hat{u}_{2} - \hat{u}_{1})^{2}}{\hat{s}_{1}^{2} + \hat{s}_{2}^{2}}
$$

### 1.2. Transform the problem

<b>Goal:</b> We need to express $J(\beta)$ as a function of $\beta$:

<b>Scatter in feature space-x:</b> Measure of how the points are spread away from the mean of a given class

$$
S_{k} = \sum_{i\in C_{k}} (x^{(i)}- u_{k})(x^{(i)}- u_{k})^{T}
$$

<b>Within-class scatter matrix:</b> Total scatter of all classes

$$
S_{W} = S_{1}+S_{2}
$$

<b>Between-class scatter matrix:</b> Distance Matrix of the class means
$$
S_{B} = (u_{2}-u_{1})(u_{2}-u_{1})^{T}
$$

<b>Objective function reformulation:</b> With these notations, we obtain:

$$
J(\beta) = \frac{\beta^{T}S_{B}\beta}{\beta^{T}S_{W}\beta}
$$

### 1.3. Solve the problem

<b>Fisher’s linear discriminant:</b> By setting the derivative to zero we obtain:

$$
\beta^{*} \propto S_{W}^{-1}(u_{2}-u_{1})
$$

## 2. Multi class

<b>Goal:</b> seek $(C − 1)$ projections $[y_{1}, y_{2}, . . . y_{C−1}]$ by means of $(C − 1)$ projection vectors $\beta_{i}$ arranged by columns into a projection matrix $\Theta = [\beta_{1}|\beta_{2}| . . . |\beta_{C−1}]$, where:

$$
y = \Theta^{T}X
$$

### 2.1. Objective function

We obtain similarly ([proof](https://pdfs.semanticscholar.org/1ab8/ea71fbef3b55b69e142897fadf43b3269463.pdf)):

$$
J(W) = \frac{\mid \Theta^{T}S_{B}\Theta \mid}{\mid \Theta^{T}S_{W}\Theta \mid}
$$

<b>Goal:</b> seek the projection matrix $\Theta^{*}$ that maximize this ratio.

<b>Fisher's criterion:</b> is maximized when the projection matrix $\Theta^{*}$ is composed of the eigenvectors of:

$$
S_{W}^{-1}S_{B}
$$

and under the assumptions that $S_{W}$ is
- <i>Non-singular matrix</i>
- <i>Invertible</i>

<b>LDA and dimensionality reduction:</b> Noticed that, there will be at most $(C−1)$ eigenvectors with non-zero real corresponding eigenvalues $λ_{i}$. This is because $S_{B}$ is of rank $(C − 1)$ or less. So we can see that LDA can represent a massive reduction in the dimensionality of the problem.

------
