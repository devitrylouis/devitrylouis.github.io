---
title: 'Agents in AI'
date: 2018-10-01
permalink: /posts/2018/11/agents-ai/
tags:
  - Artificial Intelligence
  - Basics
---

# Agents

## Introduction

The concept of rationality can be applied to a wide variety of agents.

<b>Goal:</b> Develop a small set of design principles for building successful agents—systems that can reasonably be called intelligent

<b>Addressed in this book:</b>
- Examine agents, environments, and the coupling between them.
- Give a crude categorization of environments
- How properties of an environment influence the design of suitable agents for that environment.
- Several basic “skeleton” agent designs

## Agents and environments

Definition (Agent, Environment, Sensor, Actuator): An agent is anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators.

### Percept sequence

<b>Definition: (Percept)</b> refer to the agent’s perceptual inputs at any given instant.

<b>Percept sequence</b> is the complete history of everything the agent has ever perceived

Agent’s choice of action at any given instant can depend on the entire percept sequence observed to date, but not on anything it hasn’t perceived.

It implies that by specifying the agent’s choice of action for every possible percept sequence, we have said everything there is to say about the agent.

<b>Agent function:</b> Describe the agent behavior by mapping any given percept sequence to an action .

<b>Tabulating the agent function: (External)</b> construct this table by trying out all possible percept sequences and recording which actions the agent does in response

<b>Agent program: (Internal)</b> concrete implementation, running within some physical system.

## Good behavior: The concept of rationality

### Intro

Formal definition: A rational agent is one that does the right thing—conceptually speaking, every entry in the table for the agent function is filled out correctly

How? By considering the consequences of the agent’s behavior. When an agent is plunked down in an environment, it generates a sequence of actions according to the percepts it receives. This sequence of actions causes the environment to go through a sequence of states

If the sequence is desirable, then the agent has performed well. This notion of desirability is captured by a performance measure that evaluates any given sequence of environment states. (there is not one fixed performance measure)

Rule of thumb: general rule, it is better to design performance measures according to what one actually wants in the environment, rather than according to how one thinks the agent should behave.

### Rationality

What is rational at any given time depends on four things:
- The performance measure that defines the criterion of success.
- The agent’s prior knowledge of the environment.
- The actions that the agent can perform.
- The agent’s percept sequence to date

What is rational at any given time depends on four things:
- The performance measure that defines the criterion of success.
- The agent’s prior knowledge of the environment. • The actions that the agent can perform.
- The agent’s percept sequence to date

### What is rational at any given time depends on four things:

- The performance measure that defines the criterion of success.
- The agent’s prior knowledge of the environment.
- The actions that the agent can perform.
- The agent’s percept sequence to date

### Omniscience, learning, and autonomy

<b>Distinction between rationality and omniscience</b>
- More often than not, agents can't be omniscient.
- Rationality max- imizes expected performance, while perfection maximizes actual performance.

Our definition of rationality does not require omniscience.

<b>Information gathering:</b> Doing actions in order to modify future percepts sometimes. Because looking helps maximize the expected performance.

<b>Learning</b> A rational agent not only to gather information but also to learn as much as possible from what it perceives. The agent’s initial configuration could reflect some prior knowledge of the environment, but as the agent gains experience this may be modified and augmented.

### Prior knowledge

<b>Definition: (Autonomy)</b> Wether an agent relies on the prior knowledge of its designer rather than on its own percepts.

<b>Goal:</b> A rational agent should be autonomous - it should learn what it can do to compensate for partial or incorrect prior knowledge.

<b>Agent with little to none experience:</b> it would have to act randomly unless the designer gave some assistance.

<b>Reasonable assumption:</b> provide an artificial intelligent agent with some initial knowledge as well as an ability to learn. After sufficient experience of its environment, the behavior of a rational agent can become effectively independent of its prior knowledge.

## Nature of environments

Task environments, which are essentially the “problems” to which rational agents are the “solutions.”

### Specifying the task environment

PEAS (Performance, Environment, Actuators, Sensors)

What matters is the complexity of the relationship among the behavior of the agent, the percept sequence generated by the environment, and the performance measure - not the distinction between “real” and “artificial” environments.

Software agents (or software robots or softbots) exist in rich, unlimited domains

<b>That operator will need:</b>
- Some natural language processing abilities
- The abtility to learn what each user and advertiser is interested in
- To change its plans dynamically

### Properties of task environments

Identify a fairly small number of dimensions along which task environments can be categorized.

<b>Fully observable vs. partially observable:<b>
- Fully observable
  - Convenient because the agent don't need to maintain any internal state to keep track of the world.
  - An environment might be partially observable because of noisy and inaccurate sensors or because parts of the state are simply missing from the sensor data.
- If the agent has no sensors at all then the environment is unobservable

<b>Single agent vs. multiagent<b>
- Does an agent A (the taxi driver for example) have to treat an object B (another vehicle) as an agent, or can it be treated merely as an object behaving according to the laws of physics, analogous to waves at the beach or leaves blowing in the wind? The key distinction is whether B’s behavior is best described as maximizing a performance measure whose value depends on agent A’s behavior.
- Chess is a competitive multiagent environment
partially cooperative multiagent environment
- The agent-design problems in multiagent environments are often quite different from those in single-agent en- vironments
- Communication
- Randomized behavior is rational because it avoids the pitfalls of predictability

<b>Deterministic vs. stochastic<b>
- Next state of the environment is completely deter- mined by the current state and the action executed by the agent, then we say the environment is deterministic
- If the environment is partially observable, however, then it could appear to be stochastic
- We say an environment is uncertain if it is not fully observable or not deterministic.
- “stochastic” generally implies that uncertainty about outcomes is quan- tified in terms of probabilities; a nondeterministic environment is one in which actions are characterized by their possible outcomes, but no probabilities are attached to them

<b>Episodic vs. sequential:</b>
- agent’s experience is divided into atomic episodes
- agent receives a percept and then performs a single action
- Chess and taxi driving are sequential: in both cases, short-term actions can have long-term consequences

<b>Static vs. dynamic</b>:
- Dynamic environments,
- If it hasn’t decided yet, that counts as deciding to do nothing

<b>Discrete vs. continuous</b>
- Chess: discrete
- Taxi driving: continuous

<b>Known vs. unknown</b>
- State of knowledge about the “laws of physics”
- Known environment, the outcomes (or outcome probabilities if the environment is stochastic)


# Structures of agents

## Introduction

The job of AI is to design an agent program that implements the agent function— the mapping from percepts to actions.

Computing device with physical sensors and actuators—we call this the architecture:

agent = architecture + program

program we choose has to be one that is appropriate for the architecture



------
